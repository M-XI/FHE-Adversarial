{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 For Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from qgpt2_models import QGPT2ForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sst2_gpt2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./sst2_gpt2\")\n",
    "dataset = load_dataset(\"sst2\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32407/2243020896.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_token_indexes)\n"
     ]
    }
   ],
   "source": [
    "fixed_length = 32\n",
    "\n",
    "input_sentence = dataset[20][\"sentence\"]\n",
    "\n",
    "# NOTE: To compile and then inference correctly, lengths must be fixed (we fix to length 32 to save runtime/circuit size)\n",
    "input_token_indexes = tokenizer(input_sentence, return_tensors=\"pt\", padding=\"max_length\", max_length=fixed_length, truncation=True).input_ids\n",
    "input_ids = torch.tensor(input_token_indexes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgpt2_seq = QGPT2ForSequenceClassification.from_pretrained(\"./sst2_gpt2\", n_bits=6, layers=[8, 9, 10, 11], use_cache=False)\n",
    "qgpt2_seq.set_fhe_mode(fhe=\"disable\")\n",
    "qgpt2_seq.config.pad_token_id = qgpt2_seq.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0615, -1.2352]], grad_fn=<IndexBackward0>)\n",
      "tensor([[ 1.0971, -1.3250]], grad_fn=<IndexBackward0>)\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [13:00<39:01, 780.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit compiled with at most 14 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [45:29<48:55, 1467.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit compiled with at most 15 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [1:08:16<23:41, 1421.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit compiled with at most 14 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [1:32:19<00:00, 1384.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit compiled with at most 16 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_normal = model(input_ids[:1, :]).logits\n",
    "output_q = qgpt2_seq(input_ids[:1, :]).logits\n",
    "\n",
    "print(output_normal)\n",
    "print(output_q)\n",
    "print(torch.allclose(output_normal, output_q))\n",
    "\n",
    "qgpt2_seq_circuits = qgpt2_seq.compile(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   79,   931,  5116,  2753,   281, 37959,   804,   379,   262, 29442,\n",
      "           286,  1964, 29409,   837,   475,   340,   857,   523,   351,   884,\n",
      "           281, 30690,  8216,   326,   345,  1239,   760,   618, 14733,  5645,\n",
      "           290, 13574]])\n",
      "SequenceClassifierOutputWithPast(loss=None, logits=tensor([[ 1.0971, -1.3250]], grad_fn=<IndexBackward0>), past_key_values=None, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "qgpt2_seq.set_fhe_mode(fhe=\"simulate\")\n",
    "print(input_ids)\n",
    "output_logits_simulated = qgpt2_seq(input_ids)\n",
    "print(output_logits_simulated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "attacks = {}\n",
    "attacks[\"baseline\"] = dataset[:20][\"sentence\"]\n",
    "\n",
    "budget = \"3\"\n",
    "for filename in os.listdir(\"./attacks\"):\n",
    "    with open(os.path.join(\"./attacks\", filename), \"rb\") as f:\n",
    "        attacks[filename] = pickle.load(f)[budget]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids = list(range(20))\n",
    "\n",
    "def get_inference(model, attacks, ignore_attention=False):\n",
    "    res = {}\n",
    "\n",
    "    for attack_type, attack_data in tqdm(attacks.items()):\n",
    "        res[attack_type[:-4]] = []\n",
    "        for id in ids:\n",
    "            # NOTE: some attacks generate multiple examples, we just arbitrary pick the last\n",
    "            if isinstance(attack_data[id], list):\n",
    "                attack_data[id] = attack_data[id][-1]\n",
    "            curr_input = tokenizer(attack_data[id], return_tensors='pt', padding=\"max_length\", max_length=fixed_length, truncation=True)\n",
    "            if not ignore_attention:\n",
    "                res[attack_type[:-4]].append(model(**curr_input).logits.detach().clone())\n",
    "            else:\n",
    "                res[attack_type[:-4]].append(model(curr_input.input_ids).logits.detach().clone())\n",
    "    \n",
    "    return res\n",
    "\n",
    "def get_metrics(labels, attack_inference):\n",
    "    labels = labels\n",
    "    res_acc = {}\n",
    "    res_pre = {}\n",
    "    res_rec = {}\n",
    "    crossentropy = {}\n",
    "\n",
    "    for attack_type, attack_data in attack_inference.items():\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "        for id in ids:\n",
    "            prediction = torch.argmax(attack_data[id].squeeze(), dim=-1).item()\n",
    "            if prediction == 0:\n",
    "                if labels[id] == 0:\n",
    "                    tn += 1\n",
    "                elif labels[id] == 1:\n",
    "                    fn += 1\n",
    "                else:\n",
    "                    assert False\n",
    "            elif prediction == 1:\n",
    "                if labels[id] == 0:\n",
    "                    fp += 1\n",
    "                elif labels[id] == 1:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    assert False\n",
    "            else:\n",
    "                assert False\n",
    "        \n",
    "        res_acc[attack_type] = (tp + tn) / (tp + fp + tn + fn)\n",
    "        res_pre[attack_type] = (tp) / (tp + fp)\n",
    "        res_rec[attack_type] = (tp) / (tp + fn)\n",
    "\n",
    "        curr_targets = torch.stack(attack_data, dim=0).squeeze()\n",
    "        crossentropy[attack_type] = torch.nn.functional.cross_entropy(curr_targets, torch.tensor(labels))\n",
    "    \n",
    "    return res_acc, res_pre, res_rec, crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:13<00:00,  1.01s/it]\n",
      "100%|██████████| 13/13 [03:16<00:00, 15.14s/it]\n"
     ]
    }
   ],
   "source": [
    "inference_normal = get_inference(model, attacks)\n",
    "inference_fhe = get_inference(qgpt2_seq, attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[:20][\"label\"]\n",
    "accuracy_normal, precision_normal, recall_normal, ce_normal = get_metrics(labels, inference_normal)\n",
    "accuracy_fhe, precision_fhe, recall_fhe, ce_fhe = get_metrics(labels, inference_fhe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 0.95,\n",
       " 'sst2_deletion': 0.1,\n",
       " 'sst2_deletions_targeted': 0.7,\n",
       " 'sst2_deletions_targeted_nologits': 0.6,\n",
       " 'sst2_homoglyphs': 0.35,\n",
       " 'sst2_homoglyphs_targeted': 0.7,\n",
       " 'sst2_homoglyphs_targeted_nologits': 0.7,\n",
       " 'sst2_invisibles_targeted': 0.6,\n",
       " 'sst2_invisibles_targeted_nologits': 0.65,\n",
       " 'sst2_invisible_chars': 0.5,\n",
       " 'sst2_reorder': 0.55,\n",
       " 'sst2_reorderings_targeted': 0.75,\n",
       " 'sst2_reorderings_targeted_nologits': 0.75}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 0.95,\n",
       " 'sst2_deletion': 0.15,\n",
       " 'sst2_deletions_targeted': 0.7,\n",
       " 'sst2_deletions_targeted_nologits': 0.6,\n",
       " 'sst2_homoglyphs': 0.4,\n",
       " 'sst2_homoglyphs_targeted': 0.7,\n",
       " 'sst2_homoglyphs_targeted_nologits': 0.7,\n",
       " 'sst2_invisibles_targeted': 0.6,\n",
       " 'sst2_invisibles_targeted_nologits': 0.7,\n",
       " 'sst2_invisible_chars': 0.55,\n",
       " 'sst2_reorder': 0.5,\n",
       " 'sst2_reorderings_targeted': 0.75,\n",
       " 'sst2_reorderings_targeted_nologits': 0.75}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 1.0,\n",
       " 'sst2_deletion': 0.0,\n",
       " 'sst2_deletions_targeted': 0.625,\n",
       " 'sst2_deletions_targeted_nologits': 0.5625,\n",
       " 'sst2_homoglyphs': 0.36363636363636365,\n",
       " 'sst2_homoglyphs_targeted': 0.625,\n",
       " 'sst2_homoglyphs_targeted_nologits': 0.625,\n",
       " 'sst2_invisibles_targeted': 0.5555555555555556,\n",
       " 'sst2_invisibles_targeted_nologits': 0.5882352941176471,\n",
       " 'sst2_invisible_chars': 0.5,\n",
       " 'sst2_reorder': 0.6,\n",
       " 'sst2_reorderings_targeted': 0.7272727272727273,\n",
       " 'sst2_reorderings_targeted_nologits': 1.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 1.0,\n",
       " 'sst2_deletion': 0.0,\n",
       " 'sst2_deletions_targeted': 0.625,\n",
       " 'sst2_deletions_targeted_nologits': 0.5625,\n",
       " 'sst2_homoglyphs': 0.4,\n",
       " 'sst2_homoglyphs_targeted': 0.625,\n",
       " 'sst2_homoglyphs_targeted_nologits': 0.6428571428571429,\n",
       " 'sst2_invisibles_targeted': 0.5555555555555556,\n",
       " 'sst2_invisibles_targeted_nologits': 0.625,\n",
       " 'sst2_invisible_chars': 0.5454545454545454,\n",
       " 'sst2_reorder': 0.5,\n",
       " 'sst2_reorderings_targeted': 0.7272727272727273,\n",
       " 'sst2_reorderings_targeted_nologits': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_fhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 0.9,\n",
       " 'sst2_deletion': 0.0,\n",
       " 'sst2_deletions_targeted': 1.0,\n",
       " 'sst2_deletions_targeted_nologits': 0.9,\n",
       " 'sst2_homoglyphs': 0.4,\n",
       " 'sst2_homoglyphs_targeted': 1.0,\n",
       " 'sst2_homoglyphs_targeted_nologits': 1.0,\n",
       " 'sst2_invisibles_targeted': 1.0,\n",
       " 'sst2_invisibles_targeted_nologits': 1.0,\n",
       " 'sst2_invisible_chars': 0.6,\n",
       " 'sst2_reorder': 0.3,\n",
       " 'sst2_reorderings_targeted': 0.8,\n",
       " 'sst2_reorderings_targeted_nologits': 0.5}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 0.9,\n",
       " 'sst2_deletion': 0.0,\n",
       " 'sst2_deletions_targeted': 1.0,\n",
       " 'sst2_deletions_targeted_nologits': 0.9,\n",
       " 'sst2_homoglyphs': 0.4,\n",
       " 'sst2_homoglyphs_targeted': 1.0,\n",
       " 'sst2_homoglyphs_targeted_nologits': 0.9,\n",
       " 'sst2_invisibles_targeted': 1.0,\n",
       " 'sst2_invisibles_targeted_nologits': 1.0,\n",
       " 'sst2_invisible_chars': 0.6,\n",
       " 'sst2_reorder': 0.2,\n",
       " 'sst2_reorderings_targeted': 0.8,\n",
       " 'sst2_reorderings_targeted_nologits': 0.5}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_fhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': tensor(0.1946),\n",
       " 'sst2_deletion': tensor(1.2521),\n",
       " 'sst2_deletions_targeted': tensor(0.5379),\n",
       " 'sst2_deletions_targeted_nologits': tensor(0.5383),\n",
       " 'sst2_homoglyphs': tensor(1.0868),\n",
       " 'sst2_homoglyphs_targeted': tensor(0.5803),\n",
       " 'sst2_homoglyphs_targeted_nologits': tensor(0.5113),\n",
       " 'sst2_invisibles_targeted': tensor(0.7941),\n",
       " 'sst2_invisibles_targeted_nologits': tensor(0.4755),\n",
       " 'sst2_invisible_chars': tensor(0.8643),\n",
       " 'sst2_reorder': tensor(1.0337),\n",
       " 'sst2_reorderings_targeted': tensor(0.6615),\n",
       " 'sst2_reorderings_targeted_nologits': tensor(0.5554)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': tensor(0.1954),\n",
       " 'sst2_deletion': tensor(1.2411),\n",
       " 'sst2_deletions_targeted': tensor(0.5163),\n",
       " 'sst2_deletions_targeted_nologits': tensor(0.5220),\n",
       " 'sst2_homoglyphs': tensor(1.0759),\n",
       " 'sst2_homoglyphs_targeted': tensor(0.5611),\n",
       " 'sst2_homoglyphs_targeted_nologits': tensor(0.4957),\n",
       " 'sst2_invisibles_targeted': tensor(0.7632),\n",
       " 'sst2_invisibles_targeted_nologits': tensor(0.4638),\n",
       " 'sst2_invisible_chars': tensor(0.8543),\n",
       " 'sst2_reorder': tensor(1.0290),\n",
       " 'sst2_reorderings_targeted': tensor(0.6569),\n",
       " 'sst2_reorderings_targeted_nologits': tensor(0.5602)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_fhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "display_name": "Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
